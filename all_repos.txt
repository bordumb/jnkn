
========================================
 FILE: Dockerfile
========================================
FROM python:3.11-slim
WORKDIR /app
COPY . .
RUN pip install .
ENTRYPOINT ["junkan"]


========================================
 FILE: pyproject.toml
========================================
[project]
name = "junkan"
version = "0.2.0"
description = "CI/CD Gatekeeper for Infrastructure and Data Impact Analysis"
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
    "click>=8.1.7",
    "networkx>=3.2.1",
    "sqlglot>=20.0.0",
    "tree-sitter==0.21.3",
    "tree-sitter-languages>=1.10.2",
    "pydantic>=2.5.0",
    "httpx>=0.27.0"
]

[project.scripts]
junkan = "junkan.cli:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"


========================================
 FILE: README.md
========================================
# Junkan

**The "Pre-Flight" Impact Analysis Engine for Engineering Teams.**

Junkan prevents production outages by stitching together the hidden dependencies between your **Infrastructure** (Terraform), **Data Pipelines** (dbt), and **Application Code**.

## The Problem

Most tools operate in silos:
* *Infracost* checks Terraform cost.
* *dbt* checks SQL lineage.
* *Turborepo* checks code monorepos.

**Junkan checks the "Glue".** It detects cross-domain breaking changes, like:
* **Infra $\rightarrow$ Code:** A Terraform PR renames an environment variable `DB_HOST` to `DATABASE_HOST`, silently causing your Python application to crash on startup.
* **Infra $\rightarrow$ Data:** A Terraform change rotates the IAM role for an S3 bucket, inadvertently revoking the `s3:GetObject` permission used by a dbt model to load raw CSVs, causing the nightly ETL to fail.
* **Data $\rightarrow$ Code:** A dbt model schema change renames the `user_id` column to `customer_id` in the `fct_orders` table, but the Python backend service that queries this table for the "Order History" API endpoint wasn't updated, leading to 500 errors.
* **Code $\rightarrow$ Infra:** A developer updates the `docker-compose.yml` or Kubernetes manifest to use a new Redis image version (e.g., v7), but the Terraform state still provisions an older AWS ElastiCache parameter group incompatible with v7, preventing the service from stabilizing.
* **Data $\rightarrow$ Infra:** A new dbt model logic change causes a table to triple in size overnight, exceeding the allocated storage IOPS defined in the Terraform configuration for the RDS instance, leading to severe latency.
* **Infra $\rightarrow$ Code:** A Terraform PR deletes a deprecated SQS queue, but a legacy Python background worker still has a hardcoded reference to that queue URL in its settings, causing the worker to crash loop on startup.

---

## Prerequisites

* **Python 3.11+**
* **[uv](https://github.com/astral-sh/uv)** (Required for dependency management)

## Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/your-org/junkan.git](https://github.com/your-org/junkan.git)
    cd junkan
    ```

2.  **Install dependencies:**
    Junkan uses `uv` to manage the virtual environment and dependencies instantly.
    ```bash
    uv sync
    ```

3.  **Verify installation:**
    Run the help command to ensure the CLI is ready.
    ```bash
    uv run junkan --help
    ```

---

## Quick Start

### 1. Ingest Your Artifacts
Junkan works by building a local dependency graph (stored in SQLite) from your build artifacts. It does not connect to your cloud provider; it parses the plans you generate.

**Generate standard artifacts:**
```bash
# 1. Terraform
terraform plan -out=tfplan
terraform show -json tfplan > tfplan.json

# 2. dbt
dbt compile  # Generates target/manifest.json
````

**Ingest them into Junkan:**

```bash
uv run junkan ingest \
  --tf-plan tfplan.json \
  --dbt-manifest target/manifest.json
```

*Output: `{"status": "success", "relationships_ingested": 142}`*

### 2\. Calculate Blast Radius

Once the graph is built, you can query it to see the downstream impact of changing any resource.

```bash
# Example: What happens if I modify the 'users' table?
uv run junkan blast-radius "model.users"
```

**Sample Output:**

```json
{
  "source_artifacts": ["model.users"],
  "total_impacted_count": 5,
  "impacted_artifacts": [
    "model.monthly_revenue",
    "src/payment_service/user_lookup.py",
    "aws_lambda_function.daily_report"
  ],
  "breakdown": {
    "data": ["model.monthly_revenue"],
    "code": ["src/payment_service/user_lookup.py"],
    "infra": ["aws_lambda_function.daily_report"],
    "unknown": []
  }
}
```

## Running as a GitHub Action

Junkan is designed to run in CI to block dangerous PRs.

```yaml
# .github/workflows/junkan-check.yml
name: Junkan Impact Analysis
on: [pull_request]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      # 1. Generate artifacts (Terraform/dbt)
      - run: make plan
      
      # 2. Run Junkan
      - uses: your-org/junkan@v2
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
```

## Contributing

1.  Make changes in `src/junkan/`.
2.  Run `uv sync` to update environment.
3.  Run tests (coming soon).


========================================
 FILE: .gitignore
========================================
__pycache__/
*.py[cod]
*.so
.junkan/
junkan.db
dist/
build/
*.egg-info/
.venv/
env/


========================================
 FILE: .python-version
========================================
3.12


========================================
 FILE: action.yml
========================================
name: 'Junkan Impact Analysis'
description: 'Analyze Pull Requests for cross-domain breaking changes (Infra -> Code -> Data)'
author: 'Junkan'
inputs:
  github-token:
    description: 'GitHub Token for fetching diffs'
    required: true
  working-directory:
    description: 'Root directory to scan'
    required: false
    default: '.'
outputs:
  risk-score:
    description: 'Calculated risk score (0-100)'
  impact-json:
    description: 'JSON summary of impacted artifacts'
runs:
  using: 'docker'
  image: 'Dockerfile'


========================================
 FILE: tests/e2e_live/payment_service.py
========================================
import os
import boto3

def connect():
    # CRITICAL DEPENDENCY: If this env var changes, the app crashes.
    host = os.getenv("PAYMENT_DB_HOST")
    print(f"Connecting to {host}...")


========================================
 FILE: tests/e2e_live/infra/rds.tf
========================================
resource "aws_db_instance" "payment_db_host" {
  allocated_storage    = 10
  db_name              = "mydb"
  engine               = "mysql"
  instance_class       = "db.t3.micro"
}


========================================
 FILE: scripts/concat.py
========================================
#!/usr/bin/env python3
import os
from pathlib import Path

# ------------------------------------------------------------
# CONFIGURATION
# ------------------------------------------------------------

# Skip files larger than this (e.g., 500KB) to save context window
MAX_FILE_SIZE_BYTES = 500 * 1024  

DEFAULT_IGNORE_FILES = {
    # Lock files (noisy context)
    "poetry.lock",
    "package-lock.json",
    "pnpm-lock.yaml",
    "yarn.lock",
    "Cargo.lock",
    "uv.lock",
    "Gemfile.lock",
    "go.sum",
    
    # System / IDE
    ".DS_Store",
    "Thumbs.db",
    
    # Binary / Data artifacts
    ".coverage",  # <--- This was your likely binary culprit
    "db.sqlite3",
    "dump.rdb",
}

DEFAULT_IGNORE_DIRS = {
    # Dependencies
    "node_modules",
    ".venv",
    "venv",
    "env",
    "target", # Rust
    "vendor", # Go/PHP
    
    # Build Artifacts
    "dist",
    "build",
    "out",
    ".next",
    ".turbo",
    ".mypy_cache",
    ".pytest_cache",
    ".ruff_cache",
    "__pycache__",
    "htmlcov",
    
    # VCS
    ".git",
    ".github", # Optional: keep if you want CI workflows
    ".idea",
    ".vscode",
}

DEFAULT_IGNORE_EXTENSIONS = {
    # Images/Media
    ".png", ".jpg", ".jpeg", ".gif", ".ico", ".svg", ".webp",
    ".mp4", ".mov", ".avi", ".webm", ".mp3", ".wav",
    ".pdf", ".zip", ".tar", ".gz", ".7z", ".rar",
    
    # Fonts
    ".ttf", ".otf", ".woff", ".woff2", ".eot",
    
    # Compiled/Binary
    ".pyc", ".pyo", ".pyd",
    ".exe", ".bin", ".dll", ".so", ".dylib", ".class", ".jar",
    ".pkl", ".parquet", ".onnx", ".pt", ".pth", # ML Models
    ".db", ".sqlite", ".sqlite3",
}

# ------------------------------------------------------------
# LOGIC
# ------------------------------------------------------------

def is_binary(file_path: Path) -> bool:
    """
    Heuristic to check if a file is binary.
    Reads the first 1024 bytes and looks for null bytes.
    """
    try:
        with file_path.open("rb") as f:
            chunk = f.read(1024)
            return b"\0" in chunk
    except Exception:
        return True  # If we can't read it, skip it

def concat_all(output_file="all_repos.txt"):
    """Recursively scans and concatenates text files."""
    
    root = Path(".").resolve()
    output_lines = []
    
    # Sets for fast O(1) lookups
    ignore_files = DEFAULT_IGNORE_FILES
    ignore_dirs = DEFAULT_IGNORE_DIRS
    ignore_exts = DEFAULT_IGNORE_EXTENSIONS

    print(f"üîé Scanning: {root}")
    print(f"üö´ Ignoring: binaries, lockfiles, >{MAX_FILE_SIZE_BYTES/1024:.0f}KB")

    # Use os.walk for better control over directory pruning
    for dirpath, dirnames, filenames in os.walk(root):
        
        # 1. Prune ignored directories in-place
        # We must modify 'dirnames' list to stop os.walk from entering them
        dirnames[:] = [d for d in dirnames if d not in ignore_dirs and not d.startswith(".")]

        for filename in filenames:
            file_path = Path(dirpath) / filename
            rel_path = file_path.relative_to(root)

            # 2. Skip ignored filenames
            if filename in ignore_files:
                continue

            # 3. Skip ignored extensions
            if file_path.suffix.lower() in ignore_exts:
                continue

            # 4. Skip files strictly inside ignored folders (double check for path parts)
            # This handles cases like `apps/web/.next` which might slip through top-level pruning
            if any(part in ignore_dirs for part in file_path.parts):
                continue

            # 5. Skip large files
            try:
                size = file_path.stat().st_size
                if size > MAX_FILE_SIZE_BYTES:
                    print(f"‚ö†Ô∏è  Skipping large file: {rel_path} ({size/1024:.1f} KB)")
                    continue
            except Exception:
                continue

            # 6. Binary Detection (Crucial Step)
            if is_binary(file_path):
                print(f"‚ö†Ô∏è  Skipping binary file: {rel_path}")
                continue

            # 7. Read and Append
            try:
                text = file_path.read_text(encoding="utf-8", errors="ignore")
                
                # Optional: Skip empty files
                if not text.strip():
                    continue

                output_lines.append(f"\n{'='*40}\n")
                output_lines.append(f" FILE: {rel_path}\n")
                output_lines.append(f"{'='*40}\n")
                output_lines.append(text)
                output_lines.append("\n")
                
            except Exception as e:
                print(f"‚ùå Error reading {rel_path}: {e}")

    # Write output
    out_path = root / output_file
    out_path.write_text("".join(output_lines), encoding="utf-8")
    
    print(f"\n‚úÖ Done! Scanned {len(output_lines)//5} files.")
    print(f"üìÑ Output written to: {out_path}")

if __name__ == "__main__":
    concat_all()

========================================
 FILE: scripts/verify_e2e.sh
========================================
#!/bin/bash
set -e

# Colors for dramatic effect (and readability)
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${GREEN}üß™ INITIATING E2E SURVIVAL TEST...${NC}"

# ==============================================================================
# 1. SETUP: Ensure the Codebase is Exact
# ==============================================================================
# We re-write the critical configuration files to ensure the test environment is pure.
# This prevents "it works on my machine" errors.

mkdir -p src/junkan/languages/python
mkdir -p src/junkan/languages/terraform

# Write the Python Query (How we find Env Vars)
cat << 'EOF' > src/junkan/languages/python/imports.scm
(import_statement name: (dotted_name) @import)
(import_from_statement module_name: (dotted_name) @import)

; CAPTURE: os.getenv("VAR_NAME")
(call
  function: (attribute
    object: (identifier) @obj
    attribute: (identifier) @method)
  arguments: (argument_list (string) @env_var)
  (#eq? @obj "os")
  (#match? @method "^(getenv|environ)$"))
EOF

# Write the Terraform Query (How we find Resources)
# FIX: Use 'block' node type compatible with tree-sitter-hcl
cat << 'EOF' > src/junkan/languages/terraform/resources.scm
(block
  (identifier) @block_type
  (string_lit) @res_type
  (string_lit) @res_name
  (#eq? @block_type "resource")) @resource_block
EOF

# ==============================================================================
# 2. FIXTURES: Create the "Glue" Scenario
# ==============================================================================
echo "üõ†Ô∏è  Creating Test Fixtures (The 'Payment DB' Scenario)..."
rm -rf tests/e2e_live
mkdir -p tests/e2e_live/infra

# Fixture 1: The Python Service
# It reads 'PAYMENT_DB_HOST' to connect to the DB.
cat << 'EOF' > tests/e2e_live/payment_service.py
import os
import boto3

def connect():
    # CRITICAL DEPENDENCY: If this env var changes, the app crashes.
    host = os.getenv("PAYMENT_DB_HOST")
    print(f"Connecting to {host}...")
EOF

# Fixture 2: The Terraform Infrastructure
# It provisions 'aws_db_instance.payment_db_host'.
# Notice the naming convention match (snake_case vs env var).
cat << 'EOF' > tests/e2e_live/infra/rds.tf
resource "aws_db_instance" "payment_db_host" {
  allocated_storage    = 10
  db_name              = "mydb"
  engine               = "mysql"
  instance_class       = "db.t3.micro"
}
EOF

# ==============================================================================
# 3. EXECUTION: Run the Engine
# ==============================================================================
echo "üöÄ Running Junkan High-Road Engine..."

# Reset DB
rm -f .junkan/junkan.db

# Run Scan
# We use 'uv run' to ensure we use the project's virtual environment
uv run python -m junkan.cli.main scan --dir tests/e2e_live

# ==============================================================================
# 4. VERIFICATION: Prove the Link Exists
# ==============================================================================
echo -e "\n${GREEN}üîç VERIFYING STITCHING LOGIC...${NC}"

# We query the SQLite DB directly to see if the Stitcher connected the dots.
# We are looking for an edge where:
#   Source = env:PAYMENT_DB_HOST
#   Target = file://.../rds.tf (or the resource ID)
#   Type   = reads (or similar linkage)

DB_PATH=".junkan/junkan.db"

# 1. Check if nodes exist
ENV_NODE_COUNT=$(sqlite3 $DB_PATH "SELECT count(*) FROM nodes WHERE id LIKE 'env:%PAYMENT%';")
INFRA_NODE_COUNT=$(sqlite3 $DB_PATH "SELECT count(*) FROM nodes WHERE data LIKE '%aws_db_instance%';")

if [ "$ENV_NODE_COUNT" -eq "0" ]; then
    echo -e "${RED}‚ùå FAILED: Python Env Var node not found.${NC}"
    exit 1
fi

if [ "$INFRA_NODE_COUNT" -eq "0" ]; then
    echo -e "${RED}‚ùå FAILED: Terraform Resource node not found.${NC}"
    exit 1
fi

# 2. Check the "Magic" Link (The Stitch)
# The Stitcher should have created an edge from the Env Var to the Terraform Resource
LINK_COUNT=$(sqlite3 $DB_PATH "SELECT count(*) FROM edges WHERE source LIKE 'env:%PAYMENT%' AND target LIKE '%aws_db_instance%';")

echo "---------------------------------------------------"
echo "Stats:"
echo "Env Nodes Found:   $ENV_NODE_COUNT"
echo "Infra Nodes Found: $INFRA_NODE_COUNT"
echo "Stitched Links:    $LINK_COUNT"
echo "---------------------------------------------------"

if [ "$LINK_COUNT" -gt "0" ]; then
    echo -e "${GREEN}‚úÖ SUCCESS: The system successfully bridged Code and Infrastructure!${NC}"
    echo "Proof: The Python file 'payment_service.py' is now aware of 'rds.tf'."
else
    echo -e "${RED}‚ùå FAILURE: The Stitcher did not connect the Env Var to the Terraform Resource.${NC}"
    echo "Debug Advice: Check 'src/junkan/core/stitching.py' fuzzy matching logic."
    exit 1
fi

========================================
 FILE: src/junkan/models.py
========================================
from enum import StrEnum
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field

class RelationshipType(StrEnum):
    WRITES_TO = "writes_to"
    READS_FROM = "reads_from"
    TRANSFORMS = "transforms"
    CONFIGURES = "configures"
    DEPENDS_ON = "depends_on"
    TRIGGERS = "triggers"

class ImpactRelationship(BaseModel):
    """Represents a dependency between two artifacts."""
    upstream_artifact: str
    downstream_artifact: str
    relationship_type: RelationshipType
    confidence: float = 1.0
    source: str = "manual"
    metadata: Dict[str, Any] = Field(default_factory=dict)


========================================
 FILE: src/junkan/cli.py
========================================
import click
import json
import os
from pathlib import Path
from junkan.graph.store import GraphStore
from junkan.models import ImpactRelationship, RelationshipType
from junkan.graph.parsers.terraform import TerraformParser
from junkan.graph.parsers.dbt import DbtParser
from junkan.graph.parsers.universal import UniversalParser

@click.group()
def main():
    """Junkan V2: CI/CD Gatekeeper."""
    pass

@main.command()
@click.option("--tf-plan", type=click.Path(exists=True), help="Path to tfplan.json")
@click.option("--dbt-manifest", type=click.Path(exists=True), help="Path to manifest.json")
@click.option("--code-dir", type=click.Path(exists=True), help="Directory to scan for code")  # <--- New Option
def ingest(tf_plan, dbt_manifest, code_dir):
    """Ingest artifacts into local SQLite graph."""
    store = GraphStore()
    count = 0

    # 1. Infrastructure (Terraform)
    if tf_plan:
        with open(tf_plan) as f:
            parser = TerraformParser(f.read())
            for rel in parser.parse():
                store.add_relationship(ImpactRelationship(
                    upstream_artifact=rel["upstream"],
                    downstream_artifact=rel["downstream"],
                    relationship_type=RelationshipType.CONFIGURES,
                    source="terraform"
                ))
                count += 1

    # 2. Data (dbt)
    if dbt_manifest:
        with open(dbt_manifest) as f:
            parser = DbtParser(f.read())
            for rel in parser.parse():
                store.add_relationship(ImpactRelationship(
                    upstream_artifact=rel["upstream"],
                    downstream_artifact=rel["downstream"],
                    relationship_type=RelationshipType.TRANSFORMS,
                    source="dbt"
                ))
                count += 1
    
    # 3. Application Code (Universal Parser) <--- New Logic
    if code_dir:
        click.echo(f"Scanning directory: {code_dir}")
        for root, dirs, files in os.walk(code_dir):
            # Skip hidden folders and common noise
            dirs[:] = [d for d in dirs if not d.startswith(".") and d not in ["__pycache__", "node_modules", "venv"]]
            
            for file in files:
                file_path = Path(root) / file
                # Only parse supported extensions
                if file_path.suffix in UniversalParser.EXTENSIONS:
                    try:
                        content = file_path.read_text(errors="ignore")
                        parser = UniversalParser(content, str(file_path))
                        
                        for rel in parser.parse():
                            # Normalize path to be relative to the scan root if possible
                            try:
                                downstream = str(file_path.relative_to(code_dir))
                            except ValueError:
                                downstream = str(file_path)

                            store.add_relationship(ImpactRelationship(
                                upstream_artifact=rel.get("upstream", "unknown"),
                                downstream_artifact=downstream,
                                relationship_type=RelationshipType.DEPENDS_ON,
                                source="code_scan"
                            ))
                            count += 1
                    except Exception as e:
                        pass # Skip files we can't read

    click.echo(json.dumps({"status": "success", "relationships_ingested": count}))

@main.command()
@click.argument("artifacts", nargs=-1)
def blast_radius(artifacts):
    """Calculate downstream impact for a list of artifacts."""
    store = GraphStore()
    result = store.calculate_blast_radius(list(artifacts))
    click.echo(json.dumps(result, indent=2))

if __name__ == "__main__":
    main()

========================================
 FILE: src/junkan/core/graph.py
========================================
import networkx as nx
from typing import List, Set, Optional
from .types import Node, Edge, NodeType

class DependencyGraph:
    """
    Type-safe wrapper around NetworkX.
    Decouples the graph algorithm library from the rest of the app.
    """
    def __init__(self):
        self._graph = nx.DiGraph()

    def add_node(self, node: Node):
        self._graph.add_node(node.id, data=node)

    def add_edge(self, edge: Edge):
        self._graph.add_edge(
            edge.source_id, 
            edge.target_id, 
            type=edge.type, 
            confidence=edge.confidence,
            metadata=edge.metadata
        )

    def get_node(self, node_id: str) -> Node:
        if node_id not in self._graph:
            raise KeyError(f"Node {node_id} not found")
        data = self._graph.nodes[node_id].get("data")
        if not data:
             # Fallback for ghost nodes created by edges to unscanned files
             raise KeyError(f"Node {node_id} is a ghost node (unscanned dependency)")
        return data

    def get_nodes_by_type(self, node_type: NodeType) -> List[Node]:
        """Safely iterate nodes, skipping ghosts."""
        nodes = []
        for _, data in self._graph.nodes(data=True):
            node_obj = data.get("data")
            # CRITICAL FIX: Check if node_obj exists and matches type
            if node_obj and node_obj.type == node_type:
                nodes.append(node_obj)
        return nodes

    def get_descendants(self, node_id: str) -> Set[str]:
        if node_id not in self._graph:
            return set()
        return nx.descendants(self._graph, node_id)

========================================
 FILE: src/junkan/core/types.py
========================================
from enum import StrEnum
from typing import Dict, Any, Optional
from pydantic import BaseModel, Field

class NodeType(StrEnum):
    CODE_FILE = "code_file"
    CODE_ENTITY = "code_entity"  # Class, Function
    INFRA_RESOURCE = "infra_resource"
    DATA_ASSET = "data_asset"    # Table, Topic
    UNKNOWN = "unknown"

class RelationshipType(StrEnum):
    # Structural
    CONTAINS = "contains"        # File contains Class
    IMPORTS = "imports"          # File A imports File B
    
    # Functional
    CALLS = "calls"              # Func A calls Func B
    READS = "reads"              # Code reads Data/Env
    WRITES = "writes"            # Code writes Data
    
    # Infrastructure
    PROVISIONS = "provisions"    # TF creates Resource
    CONFIGURES = "configures"    # Resource A depends on B

class Node(BaseModel):
    """Universal Unit of Analysis."""
    id: str  # Unique Identifier (e.g., "file://src/main.py")
    name: str
    type: NodeType
    path: Optional[str] = None
    language: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        frozen = True  # Make hashable

class Edge(BaseModel):
    """Directed relationship between two Nodes."""
    source_id: str
    target_id: str
    type: RelationshipType
    confidence: float = 1.0
    metadata: Dict[str, Any] = Field(default_factory=dict)

========================================
 FILE: src/junkan/core/stitching.py
========================================
from abc import ABC, abstractmethod
from typing import List
from .graph import DependencyGraph
from .types import NodeType, RelationshipType, Edge

class StitchingRule(ABC):
    """Abstract Strategy for linking nodes across domains."""
    @abstractmethod
    def apply(self, graph: DependencyGraph) -> List[Edge]:
        pass

class EnvVarToInfraRule(StitchingRule):
    """
    Heuristic: Links code reading 'DB_HOST' to Terraform resources named 'db_host'.
    """
    def apply(self, graph: DependencyGraph) -> List[Edge]:
        edges = []
        
        # 1. Get all Env Var nodes (created by the Parser)
        env_nodes = [
            n for n in graph.get_nodes_by_type(NodeType.DATA_ASSET) 
            if n.id.startswith("env:")
        ]
        
        # 2. Get all Infra nodes (created by Terraform Parser)
        infra_nodes = graph.get_nodes_by_type(NodeType.INFRA_RESOURCE)
        
        for env in env_nodes:
            # Clean name: "DB_HOST" -> "dbhost"
            env_clean = env.name.lower().replace("_", "")
            
            for infra in infra_nodes:
                # Clean name: "aws_db_instance.main" -> "awsdbinstancemain"
                infra_clean = infra.name.lower().replace("_", "").replace(".", "")
                
                # Fuzzy Match
                if env_clean in infra_clean:
                    edges.append(Edge(
                        source_id=env.id,
                        target_id=infra.id,
                        type=RelationshipType.READS,
                        confidence=0.7,
                        metadata={"rule": "EnvVarToInfraFuzzy"}
                    ))
                    
        return edges

class Stitcher:
    def __init__(self):
        self.rules: List[StitchingRule] = [
            EnvVarToInfraRule()
        ]

    def stitch(self, graph: DependencyGraph):
        for rule in self.rules:
            new_edges = rule.apply(graph)
            for edge in new_edges:
                graph.add_edge(edge)

========================================
 FILE: src/junkan/core/storage/memory.py
========================================
from .base import StorageAdapter
from ..types import Node, Edge

class MemoryStorage(StorageAdapter):
    """Ephemeral storage for fast testing or CI runs."""
    def __init__(self):
        self.nodes = {}
        self.edges = []

    def save_node(self, node: Node):
        self.nodes[node.id] = node

    def save_edge(self, edge: Edge):
        self.edges.append(edge)

========================================
 FILE: src/junkan/core/storage/sqlite.py
========================================
import sqlite3
import json
from pathlib import Path
from .base import StorageAdapter
from ..types import Node, Edge

class SQLiteStorage(StorageAdapter):
    """Persistent storage using local SQLite file."""
    
    def __init__(self, db_path: Path):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS nodes (
                    id TEXT PRIMARY KEY,
                    data JSON
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS edges (
                    source TEXT,
                    target TEXT,
                    type TEXT,
                    data JSON,
                    PRIMARY KEY (source, target, type)
                )
            """)

    def save_node(self, node: Node):
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO nodes VALUES (?, ?)",
                (node.id, node.json())
            )

    def save_edge(self, edge: Edge):
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO edges VALUES (?, ?, ?, ?)",
                (edge.source_id, edge.target_id, edge.type, edge.json())
            )

========================================
 FILE: src/junkan/core/storage/base.py
========================================
from abc import ABC, abstractmethod
from ..types import Node, Edge

class StorageAdapter(ABC):
    """Interface for persistence strategies (SQLite, Memory, Neptune)."""
    
    @abstractmethod
    def save_node(self, node: Node):
        pass

    @abstractmethod
    def save_edge(self, edge: Edge):
        pass

========================================
 FILE: src/junkan/analysis/blast_radius.py
========================================
from typing import Dict, List, Any
from ..core.graph import DependencyGraph

class BlastRadiusAnalyzer:
    def __init__(self, graph: DependencyGraph):
        self.graph = graph

    def calculate(self, changed_artifacts: List[str]) -> Dict[str, Any]:
        unique_downstream = set()
        
        # Normalize inputs to IDs if possible
        # (For Phase 1, we assume input is a partial ID match)
        
        for root_id in changed_artifacts:
            # Try to find strict match, or fuzzy match
            try:
                # Strict check
                self.graph.get_node(root_id)
                descendants = self.graph.get_descendants(root_id)
                unique_downstream.update(descendants)
            except KeyError:
                # Fallback: Treat as file path
                file_id = f"file://{root_id}"
                descendants = self.graph.get_descendants(file_id)
                unique_downstream.update(descendants)

        return {
            "source_artifacts": changed_artifacts,
            "total_impacted_count": len(unique_downstream),
            "impacted_artifacts": list(unique_downstream),
        }

========================================
 FILE: src/junkan/graph/store.py
========================================
import sqlite3
import json
import networkx as nx
from pathlib import Path
from typing import List, Dict, Any
from junkan.models import ImpactRelationship

DB_PATH = Path(".junkan/junkan.db")

class GraphStore:
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self.graph = nx.DiGraph()
        self._init_db()
        self._load_from_db()

    def _init_db(self):
        if not self.db_path.parent.exists():
            self.db_path.parent.mkdir(parents=True)
        
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        # Simple schema: distinct nodes and directed edges
        cur.execute("""
            CREATE TABLE IF NOT EXISTS edges (
                upstream TEXT,
                downstream TEXT,
                type TEXT,
                metadata JSON,
                PRIMARY KEY (upstream, downstream, type)
            )
        """)
        conn.commit()
        conn.close()

    def _load_from_db(self):
        """Hydrate NetworkX graph from SQLite."""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        rows = cur.execute("SELECT upstream, downstream, type, metadata FROM edges").fetchall()
        for u, d, t, m in rows:
            self.graph.add_edge(u, d, relationship_type=t, metadata=json.loads(m))
        conn.close()

    def add_relationship(self, rel: ImpactRelationship):
        """Write through to DB and update in-memory graph."""
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # Upsert logic (SQLite specific)
        cur.execute("""
            INSERT OR REPLACE INTO edges (upstream, downstream, type, metadata)
            VALUES (?, ?, ?, ?)
        """, (rel.upstream_artifact, rel.downstream_artifact, rel.relationship_type, json.dumps(rel.metadata)))
        
        conn.commit()
        conn.close()
        
        self.graph.add_edge(
            rel.upstream_artifact, 
            rel.downstream_artifact, 
            relationship_type=rel.relationship_type,
            metadata=rel.metadata
        )

    def calculate_blast_radius(self, changed_artifacts: List[str]) -> Dict[str, Any]:
        """Core Impact Analysis Logic."""
        unique_downstream = set()
        
        for root in changed_artifacts:
            if root in self.graph:
                descendants = nx.descendants(self.graph, root)
                unique_downstream.update(descendants)

        # Categorize results
        breakdown = {"infra": [], "data": [], "code": [], "unknown": []}
        for art in unique_downstream:
            if any(x in art for x in ["aws_", "google_", "azure_", "k8s"]):
                breakdown["infra"].append(art)
            elif any(x in art for x in ["table", "model", "view"]):
                breakdown["data"].append(art)
            elif art.endswith((".py", ".ts", ".js", ".go")):
                breakdown["code"].append(art)
            else:
                breakdown["unknown"].append(art)

        return {
            "source_artifacts": changed_artifacts,
            "total_impacted_count": len(unique_downstream),
            "impacted_artifacts": list(unique_downstream),
            "breakdown": breakdown
        }


========================================
 FILE: src/junkan/graph/parsers/terraform.py
========================================
import json
from typing import Dict, Any, List

class TerraformParser:
    """Parses Terraform Plan JSON (Schema v1)."""
    def __init__(self, plan_json_content: str):
        try:
            self.data = json.loads(plan_json_content)
        except json.JSONDecodeError:
            self.data = {}

    def parse(self) -> List[Dict[str, Any]]:
        relationships = []
        config = self.data.get("configuration", {})
        root_module = config.get("root_module", {})
        resources = root_module.get("resources", [])

        for res in resources:
            addr = res.get("address")
            
            # Explicit depends_on
            for dep in res.get("depends_on", []):
                relationships.append({
                    "upstream": dep,
                    "downstream": addr,
                    "type": "configures"
                })

            # Implicit references
            expressions = res.get("expressions", {})
            for attr, expr in expressions.items():
                for ref in expr.get("references", []):
                    if ref != addr and "var." not in ref:
                        relationships.append({
                            "upstream": ref,
                            "downstream": addr,
                            "type": "configures"
                        })
        return relationships


========================================
 FILE: src/junkan/graph/parsers/dbt.py
========================================
import json
from typing import Dict, Any, List

class DbtParser:
    """Parses dbt manifest.json."""
    def __init__(self, content: str):
        try:
            self.data = json.loads(content)
        except json.JSONDecodeError:
            self.data = {}

    def parse(self) -> List[Dict[str, Any]]:
        relationships = []
        nodes = self.data.get("nodes", {})
        
        for key, node in nodes.items():
            downstream = node.get("name")
            # Upstream refs
            for node_id in node.get("depends_on", {}).get("nodes", []):
                # dbt node IDs are usually 'model.project.name'
                # We try to extract just the name for cleaner matching
                upstream = node_id.split(".")[-1]
                relationships.append({
                    "upstream": upstream,
                    "downstream": downstream,
                    "type": "transforms"
                })
        return relationships


========================================
 FILE: src/junkan/graph/parsers/universal.py
========================================
# junkan/graph/parsers/universal.py
from pathlib import Path
from typing import List, Dict, Any, Optional
from tree_sitter_languages import get_language, get_parser

class UniversalParser:
    EXTENSIONS = {
        ".py": "python", ".go": "go", ".js": "javascript", ".ts": "typescript"
    }
    
    # Tree-sitter queries to extract import names
    QUERIES = {
        "python": """
            (import_from_statement module_name: (dotted_name) @import)
            (import_from_statement module_name: (relative_import) @import)
            (import_statement name: (dotted_name) @import)
        """,
        "javascript": """
            (import_statement source: (string) @import)
        """,
        "typescript": """
            (import_statement source: (string) @import)
        """,
        "go": """
            (import_spec path: (interpreted_string_literal) @import)
        """
    }

    def __init__(self, content: str, file_path: str):
        self.content = content
        self.file_path = Path(file_path)
        self.ext = self.file_path.suffix.lower()
        self.lang = self.EXTENSIONS.get(self.ext)

    def parse(self) -> List[Dict[str, Any]]:
        rels = []
        if not self.lang: return rels
        
        try:
            parser = get_parser(self.lang)
            language = get_language(self.lang)
            tree = parser.parse(bytes(self.content, "utf8"))
            
            query = language.query(self.QUERIES.get(self.lang, ""))
            captures = query.captures(tree.root_node)

            for node, capture_name in captures:
                if capture_name == "import":
                    # 1. Extract raw text (e.g., "junkan.models")
                    raw_import = node.text.decode("utf8").strip("'\"")
                    
                    # 2. Resolve to a likely file path (e.g., "junkan/models.py")
                    upstream = self._resolve(raw_import)
                    
                    rels.append({
                        "upstream": upstream,
                        "metadata": {"raw": raw_import}
                    })
        except Exception as e:
            # Swallow parsing errors for robust scanning
            pass
            
        return rels

    def _resolve(self, raw_import: str) -> str:
        """Simple heuristic to map imports to file paths."""
        if self.lang == "python":
            # Map 'junkan.models' -> 'junkan/models.py'
            return raw_import.replace(".", "/") + ".py"
        return raw_import

========================================
 FILE: src/junkan/cli/main.py
========================================
import click
import json
from pathlib import Path
from ..core.graph import DependencyGraph
from ..core.stitching import Stitcher
from ..core.storage.sqlite import SQLiteStorage
from ..languages.parser import TreeSitterEngine, LanguageConfig
from ..analysis.blast_radius import BlastRadiusAnalyzer

# Initialize Engine
engine = TreeSitterEngine()

# Register Languages
# We use the path relative to this file to find the .scm queries
BASE_DIR = Path(__file__).resolve().parent.parent
engine.register_language(LanguageConfig(
    name="python",
    extensions=[".py"],
    query_path=BASE_DIR / "languages/python/imports.scm"
))

# Note: For Terraform, we map .tf to 'hcl' parser in tree-sitter-languages
engine.register_language(LanguageConfig(
    name="hcl", 
    extensions=[".tf"],
    query_path=BASE_DIR / "languages/terraform/resources.scm"
))

@click.group()
def main():
    """Junkan V2: High Road Architecture."""
    pass

@main.command()
@click.option("--dir", default=".", help="Codebase root to scan")
@click.option("--db", default=".junkan/junkan.db", help="Path to SQLite DB")
def scan(dir, db):
    """
    Phase 1 & 3: Parse and Stitch.
    Builds the graph from source code and saves to SQLite.
    """
    graph = DependencyGraph()
    storage = SQLiteStorage(Path(db))
    
    root = Path(dir)
    click.echo(f"üöÄ Scanning {root} with High-Road Engine...")

    # 1. Parsing Phase
    count_nodes = 0
    count_edges = 0
    
    # Walk the directory
    for path in root.rglob("*"):
        if path.is_file() and ".junkan" not in path.parts:
            # The parser yields both Nodes and Edges
            for result in engine.parse_file(path):
                if hasattr(result, "source_id"): # It's an Edge
                    graph.add_edge(result)
                    storage.save_edge(result)
                    count_edges += 1
                else: # It's a Node
                    graph.add_node(result)
                    storage.save_node(result)
                    count_nodes += 1
    
    click.echo(f"‚úÖ Parsed {count_nodes} nodes and {count_edges} edges.")

    # 2. Stitching Phase
    click.echo("üßµ Stitching cross-domain dependencies...")
    stitcher = Stitcher()
    stitcher.stitch(graph)
    
    # Save stitched edges
    # (In a real app, stitcher would return new edges to save)
    # For now, we assume graph is updated in memory, and we should persist the new state
    # This is an optimization for Phase 4.
    
    click.echo("‚úÖ Scan Complete.")

@main.command()
@click.argument("artifacts", nargs=-1)
@click.option("--db", default=".junkan/junkan.db", help="Path to SQLite DB")
def blast_radius(artifacts, db):
    """
    Phase 4: Query the Graph.
    Calculates downstream impact using the SQLite data.
    """
    # Load Graph from DB (Hydration)
    # Note: In a real persistent CLI, we'd add a 'load' method to Graph/Storage
    # For this snippet, we'll scan in memory or assume hydration implementation.
    # To keep this DRY, let's just warn:
    
    click.echo("‚ö†Ô∏è  Persistence loading is Phase 4. For now, run scan + analyze in one go or implement hydration.")
    
    # To make this fully functional right now, we'd need to hydrate the graph here.
    # But you have 100% of the scaffolding logic above.

if __name__ == "__main__":
    main()

========================================
 FILE: src/junkan/languages/parser.py
========================================
import sys
from pathlib import Path
from typing import Dict, List, Generator, Optional
from tree_sitter_languages import get_language, get_parser
from ..core.types import Node, Edge, NodeType, RelationshipType

class LanguageConfig:
    def __init__(self, name: str, extensions: List[str], query_path: Path):
        self.name = name
        self.extensions = set(extensions)
        self.query_path = query_path

class TreeSitterEngine:
    def __init__(self):
        self._configs: Dict[str, LanguageConfig] = {}

    def register_language(self, config: LanguageConfig):
        self._configs[config.name] = config

    def supports(self, file_path: Path) -> Optional[str]:
        ext = file_path.suffix.lower()
        for name, config in self._configs.items():
            if ext in config.extensions:
                return name
        return None

    def parse_file(self, file_path: Path) -> Generator[Edge | Node, None, None]:
        lang_name = self.supports(file_path)
        if not lang_name:
            return

        config = self._configs[lang_name]
        try:
            content = file_path.read_bytes()
            parser = get_parser(lang_name)
            tree = parser.parse(content)
            
            # 1. Yield the File Node
            file_id = f"file://{file_path}"
            yield Node(
                id=file_id, 
                name=file_path.name, 
                type=NodeType.CODE_FILE, 
                path=str(file_path),
                language=lang_name
            )

            if not config.query_path.exists():
                return
                
            query_scm = config.query_path.read_text()
            language = get_language(lang_name)
            query = language.query(query_scm)
            captures = query.captures(tree.root_node)

            for node, capture_name in captures:
                text = node.text.decode("utf-8")
                # CLEANING: Strip quotes immediately for cleaner data
                clean_text = text.strip('"\'')
                
                if capture_name == "import":
                    target_id = self._resolve_import(clean_text, lang_name)
                    # Yield Virtual Node
                    yield Node(
                        id=target_id,
                        name=clean_text,
                        type=NodeType.UNKNOWN,
                        metadata={"virtual": True}
                    )
                    yield Edge(
                        source_id=file_id,
                        target_id=target_id,
                        type=RelationshipType.IMPORTS
                    )
                
                elif capture_name == "env_var":
                    env_id = f"env:{clean_text}"
                    yield Node(
                        id=env_id, 
                        name=clean_text, 
                        type=NodeType.DATA_ASSET,
                        metadata={"source": "env_var"}
                    )
                    yield Edge(
                        source_id=file_id, 
                        target_id=env_id, 
                        type=RelationshipType.READS
                    )

                # Terraform: Capture precise name if available
                elif capture_name == "res_name":
                     yield Node(
                        id=f"infra:{clean_text}",
                        name=clean_text,
                        type=NodeType.INFRA_RESOURCE
                    )
                
                # Terraform: Fallback to block if query is older/generic
                elif capture_name == "resource_block":
                    yield Node(
                        id=f"infra:{text}", # Keep raw text for block ID to be unique
                        name=text,
                        type=NodeType.INFRA_RESOURCE
                    )
        except Exception as e:
            # Swallow errors to keep scanning
            print(f"‚ö†Ô∏è  Error parsing {file_path}: {e}", file=sys.stderr)

    def _resolve_import(self, raw_import: str, lang: str) -> str:
        clean = raw_import.strip("'\"")
        if lang == "python":
            return f"file://{clean.replace('.', '/')}.py"
        return f"file://{clean}"

========================================
 FILE: src/junkan/languages/terraform/resources.scm
========================================
(block
  (identifier) @block_type
  (string_lit) @res_type
  (string_lit) @res_name
  (#eq? @block_type "resource")) @resource_block


========================================
 FILE: src/junkan/languages/python/definitions.scm
========================================
; Capture Function Definitions
(function_definition
  name: (identifier) @definition)

; Capture Class Definitions
(class_definition
  name: (identifier) @definition)

========================================
 FILE: src/junkan/languages/python/imports.scm
========================================
(import_statement name: (dotted_name) @import)
(import_from_statement module_name: (dotted_name) @import)

; CAPTURE: os.getenv("VAR_NAME")
(call
  function: (attribute
    object: (identifier) @obj
    attribute: (identifier) @method)
  arguments: (argument_list (string) @env_var)
  (#eq? @obj "os")
  (#match? @method "^(getenv|environ)$"))

